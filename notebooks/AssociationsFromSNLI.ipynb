{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Free Association Norms to SNLI-derived association"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Peter's DT-RNN model trained on the SNLI data set to predict one-word entailments for words in the Free Association Norms dataset. Only those words that are contained in both the SNLI dataset and the Free Association Norms are analyzed. Over 80% of words in norms are also present in the SNLI dataset.\n",
    "\n",
    "Premature conclusion: One word entailings produce a word that is associate of a given word in less than 10% of tested cases. Reasons as to why this is the case are outlined at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pysem\n",
    "import pickle\n",
    "\n",
    "from collections import namedtuple\n",
    "from pysem.corpora import SNLI\n",
    "from pysem.networks import DependencyNetwork\n",
    "from pysem.generatives import EmbeddingGenerator, EncoderDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the DT-RNN model pretrained on the SNLI dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TrainingPair = namedtuple('TrainingPair', ['sentence1', 'sentence2', 'label'])  # needed for pickle\n",
    "\n",
    "with open('../data/train_data.pickle', 'rb') as pfile:\n",
    "    train_data = pickle.load(pfile)\n",
    "    \n",
    "with open('../data/vocab.pickle', 'rb') as pfile:\n",
    "    vocab_snli = pickle.load(pfile)\n",
    "    \n",
    "model = EncoderDecoder(encoder=None, decoder=None, data=train_data)\n",
    "model.load('enc_model_0006_alt.pickle','dec_model_0006_alt.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading free-associations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sparat\n",
    "path = '/home/ivana/phd/workspace/sparat/data/associationmatrices/'\n",
    "name = 'freeassoc_asymmetric'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load one vocabulary containig all words in Free Norms, and another one with animal words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat, vocab_free_full, w2i = sparat.load_assoc_mat(path, name)\n",
    "vocab_free_animals = [w.lower().strip() for w in open('../data/animal_words.txt','r').readlines()]\n",
    "\n",
    "vocab_snli = [w.lower() for w in vocab_snli]\n",
    "vocab_free_full = [w.lower() for w in vocab_free_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stairway', 'life', 'unsure', 'loft', 'tease', 'flow']\n",
      "['aardvark', 'alligator', 'ant', 'anteater']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_free_full[:6])\n",
    "print(vocab_free_animals[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons of vocabulary sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the number of words in all vocabularies (SNLI, norms all, norms animals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in a vocabulary\n",
      "---\n",
      "SNLI vocab: 19306\n",
      "Free associations vocab: 5018\n",
      "Animals in free associations vocab: 158\n"
     ]
    }
   ],
   "source": [
    "print('Total number of words in a vocabulary\\n---')\n",
    "print('SNLI vocab:', len(np.unique(vocab_snli)))\n",
    "print('Free associations vocab:', len(vocab_free_full))\n",
    "print('Animals in free associations vocab:', len(vocab_free_animals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SNLI vocabulary has almost four times more words, but many of those are non-words like different numbers (1.9, 100th), expressions (100-meter, 12-hour), random words (ab, abc. ) and various grammatical forms of the same word. In other words, it contains a lot of noise.\n",
    "\n",
    "Now let's compare the number of words contained in both vocabularies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words found in free norms vocab and in SNLI vocab: 4208\n",
      "Number of animals in both datasets: 146\n"
     ]
    }
   ],
   "source": [
    "words_overlap = list(set(vocab_free_full) & set(vocab_snli))\n",
    "print('Number of words found in free norms vocab and in SNLI vocab:', len(words_overlap))\n",
    "\n",
    "animals_overlap = list(set(vocab_free_animals) & set(vocab_snli))\n",
    "print('Number of animals in both datasets:', len(animals_overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in free norms but not in SNLI: 810\n"
     ]
    }
   ],
   "source": [
    "print('Words in free norms but not in SNLI:', len(set(vocab_free_full) - set(vocab_snli)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 15% of words in Norms are not present in the SNLI vocabulary. Eyeballing the data, looks like a lot of words related to food and animals are not included, any words with potentially promiment affective connotations as well as more technical/domain-specific terminology (eg. in economics, medicine, law)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obsession', 'jock', 'possibility', 'rhyme', 'poise']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(vocab_free_full)-set(vocab_snli))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating one-word entailments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Pete's trained model and explore what comes out at the side of decoder when a single word is presented. First testing frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ['cat','dog', 'fish', 'octopus', 'eat', 'mother', 'son', 'book', 'happiness', \n",
    "         'luck','when', 'why', 'how', 'eat', 'banana']\n",
    "\n",
    "for word in words:\n",
    "    assert word in words_overlap, word + ' not in one of the vocabularies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded -> Decoded:\n",
      "---\n",
      "cat -> cat cat furry with cat\n",
      "dog -> dog dog leashed with dog\n",
      "fish -> fish fish fishing near fish\n",
      "octopus -> octopus whale whale underwater octopus\n",
      "eat -> eating eat eating of eat\n",
      "mother -> mother mother distraught with mother\n",
      "son -> father son newborn than son\n",
      "book -> book book paperback about book\n",
      "happiness -> marriage affection happy in happiness\n",
      "luck -> fortune gamble lucky of luck\n",
      "when -> it later about about other\n",
      "why -> what answer sad about reasons\n",
      "how -> what react about about importance\n",
      "eat -> eating eat eating of eat\n",
      "banana -> banana banana nutritious of banana\n"
     ]
    }
   ],
   "source": [
    "print('Encoded -> Decoded:\\n---')\n",
    "for word in words:\n",
    "    model.encode(word)\n",
    "    print(word, '->', model.decode('dog is similar to cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the very frequent words are just repetitions of themelves (cat->cat, dog->dog). The situation gets a bit more interesting when inspecting less frequent words and interrogative words. \n",
    "\n",
    "We now analyse in how many cases the entailed word appears in the list of associates for a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded word: ease\n",
      "Decoded: soothe\n",
      "Associates: ['easy', 'relax', 'hard', 'difficult', 'simple', 'slow', 'comfort', 'tense', 'pain', 'soothe', 'help', 'smooth', 'slide', 'ability', 'quick', 'nervous', 'grace', 'complex', 'mind', 'rest'] \n",
      "\n",
      "Encoded word: obnoxious\n",
      "Decoded: annoying\n",
      "Associates: ['rude', 'loud', 'annoying', 'jerk', 'snotty', 'irritating', 'crazy', 'stupid', 'pain', 'me', 'snob', 'drunk', 'mean', 'bother', 'attitude', 'people', 'arrogant', 'silly', 'nose'] \n",
      "\n",
      "Encoded word: poor\n",
      "Decoded: poverty\n",
      "Associates: ['rich', 'bad', 'people', 'old', 'money', 'dirty', 'poverty', 'house', 'lonely', 'welfare', 'sad'] \n",
      "\n",
      "Encoded word: daily\n",
      "Decoded: everyday\n",
      "Associates: ['news', 'weekly', 'routine', 'newspaper', 'everyday', 'always', 'paper', 'today', 'monthly', 'day', 'bread', 'regular', 'often', 'log', 'ritual', 'schedule'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_assoc(word,top=1):\n",
    "    word = word.upper()\n",
    "    row = mat[w2i[word]]\n",
    "    row[w2i[word]] = 0 # auto-assoc\n",
    "    if top == 1:\n",
    "        idx = np.argmax(row)\n",
    "        return vocab_free_full[idx].lower()\n",
    "    else:\n",
    "        non_zeros = len(np.nonzero(row)[0])\n",
    "        idx = np.argsort(row)[-non_zeros:]\n",
    "        return [vocab_free_full[i].lower() for i in idx[::-1]]\n",
    "\n",
    "counter = 0\n",
    "for word in words_overlap:\n",
    "    model.encode(word)\n",
    "    decoded=model.decode('word')\n",
    "    if word!=decoded:\n",
    "        list_assoc = get_assoc(word, 0)\n",
    "        if decoded in list_assoc:\n",
    "            counter += 1\n",
    "            if counter < 5:\n",
    "                print('Encoded word:', word)\n",
    "                print('Decoded:', decoded)\n",
    "                print('Associates:', list_assoc, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entailed word is an associate in less than 10% of cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08745247148288973"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter/len(words_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible reason why there is such a small overlap in entailment predictions and free norms might be the dataset that is used for training of the model. SNLI dataset contains sentences which usually involve people doing certain actions, generated by captioning imges. The vocabulary gathered in that way does not contain many finesses arising in a discourse or a written piece of text. To alleviate this one could try training the model on a different data set.\n",
    "\n",
    "Another reason for this poor match might be the model architecture which is designed as to process syntax, something less relevant in free association norms. So, the second idea is to modify the model to train it on more co-occurrence data (although some people have strongly argued against the interpretation of associations as co-occurences in text)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
